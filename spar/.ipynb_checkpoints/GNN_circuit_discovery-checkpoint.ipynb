{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6366b7-709e-4f1f-846f-97c7103a1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf5a91a-6ea5-4c3c-ae29-5c60bb461bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.dirname(os.path.abspath(\"GNN_circuit_discovery.ipynb\"))\n",
    "model_dir = os.path.join(current_dir, '..') # Assuming it's one level up\n",
    "data_dir = os.path.join(model_dir, '..')\n",
    "#toy_model_dir = os.path.join(current_dir, '..', 'llm_from_scratch/LLM_from_scratch/')\n",
    "\n",
    "sys.path.append(model_dir)\n",
    "sys.path.append(data_dir)\n",
    "#sys.path.append(toy_model_dir)\n",
    "\n",
    "from config.gpt.training import options\n",
    "from config.sae.models import sae_options\n",
    "from models.gpt import GPT\n",
    "from models.sparsified import SparsifiedGPT\n",
    "from data.tokenizers import ASCIITokenizer, TikTokenTokenizer\n",
    "\n",
    "#from utils import generate\n",
    "c_name = 'standardx8.shakespeare_64x4'\n",
    "name = 'standard.shakespeare_64x4'\n",
    "config = sae_options[c_name]\n",
    "\n",
    "model = SparsifiedGPT(config)\n",
    "model_path = os.path.join(\"../checkpoints\", name)\n",
    "model = model.load(model_path, device=config.device)\n",
    "\n",
    "tokenizer = ASCIITokenizer() if \"shakespeare\" in name else TikTokenTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f401a5-061e-47e4-b916-09b573d77b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0afbc921-968c-429c-bcdc-43daf50a8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a small set of text prompts from training data\n",
    "raw_text_prompts = []\n",
    "\n",
    "with open('/Volumes/MacMini/gpt-circuits/data/shakespeare/input.txt', 'r') as file:\n",
    "    for _ in range(64):\n",
    "        line = file.readline()\n",
    "        raw_text_prompts.append(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae14c34-6d59-4b16-b462-bd4c37bb58d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fbe2426-1ea1-41ad-9ae8-c2a26092ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation \n",
    "def prepare_dataset(raw_text_prompts, tokenizer, batch_size=16):\n",
    "    # Tokenize all prompts\n",
    "    tokenized_data = []\n",
    "    for prompt in raw_text_prompts:\n",
    "        tokens = tokenizer.encode(prompt)\n",
    "        tokens = torch.Tensor(tokens).long().unsqueeze(0)\n",
    "        \n",
    "        tokenized_data.append({\n",
    "            'input_ids': tokens,\n",
    "            'attention_mask': tokens['attention_mask'].squeeze()\n",
    "        })\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TokenizedDataset(tokenized_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def collect_GAT_training_data(model, dataloader, tokenizer):\n",
    "    '''Example data for each batch:\n",
    "        {\n",
    "        'input_ids': tensor[batch_size, seq_len],  # Input token IDs\n",
    "        'sae_features': [\n",
    "            # List of tensors, one per SAE layer\n",
    "            tensor[batch_size, num_features_layer1],  # Activations from SAE layer 1\n",
    "            tensor[batch_size, num_features_layer2],  # Activations from SAE layer 2\n",
    "            # ...and so on for all SAE layers\n",
    "        ],\n",
    "        'logits': tensor[batch_size, vocab_size]  # Original model output logits\n",
    "        }\n",
    "        \n",
    "        '''\n",
    "    training_data = []\n",
    "    \n",
    "    # Register hooks to capture SAE activations\n",
    "    #activation_hooks = register_sae_hooks(model)\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        random_prompt = \"a\"\n",
    "        tokens = tokenizer.encode(random_prompt)\n",
    "        tokens = torch.Tensor(tokens).long().unsqueeze(0)\n",
    "        \n",
    "        # Forward pass through model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            \n",
    "            # Get output logits\n",
    "            logits = outputs.logits[:, -1, :]  # Last token prediction\n",
    "            \n",
    "            # Get SAE activations from hooks\n",
    "            sae_activations = [hook.activations for hook in activation_hooks]\n",
    "            \n",
    "            # Store this batch's data\n",
    "            batch_data = {\n",
    "                'input_ids': input_ids,\n",
    "                'sae_features': sae_activations,\n",
    "                'logits': logits\n",
    "            }\n",
    "            \n",
    "            training_data.append(batch_data)\n",
    "            \n",
    "            # Clear hook activations for next batch\n",
    "            for hook in activation_hooks:\n",
    "                hook.clear()\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7ac005-9a08-4dce-bb9f-80774dee26b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ASCIITokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m prepare_dataset(raw_text_prompts, tokenizer)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(raw_text_prompts, tokenizer, batch_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenized_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m raw_text_prompts:\n\u001b[0;32m----> 6\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m      7\u001b[0m         prompt,\n\u001b[1;32m      8\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     11\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     tokenized_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(),\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     16\u001b[0m     })\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ASCIITokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "dataloader = prepare_dataset(raw_text_prompts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5f805-6cb4-4524-8fc8-fabdc8c9a31b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
