{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fbe2426-1ea1-41ad-9ae8-c2a26092ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_training_data(model, dataloader):\n",
    "    training_data = []\n",
    "    \n",
    "    # Register hooks to capture SAE activations\n",
    "    activation_hooks = register_sae_hooks(model)\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        \n",
    "        # Forward pass through model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            \n",
    "            # Get output logits\n",
    "            logits = outputs.logits[:, -1, :]  # Last token prediction\n",
    "            \n",
    "            # Get SAE activations from hooks\n",
    "            sae_activations = [hook.activations for hook in activation_hooks]\n",
    "            \n",
    "            # Store this batch's data\n",
    "            batch_data = {\n",
    "                'input_ids': input_ids,\n",
    "                'sae_features': sae_activations,\n",
    "                'logits': logits\n",
    "            }\n",
    "            \n",
    "            training_data.append(batch_data)\n",
    "            \n",
    "            # Clear hook activations for next batch\n",
    "            for hook in activation_hooks:\n",
    "                hook.clear()\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "def train_circuit_gnn(gnn_model, training_data, l1_weight=0.01, epochs=5):\n",
    "    optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in training_data:\n",
    "            # Flatten all SAE features into a single feature list\n",
    "            all_features = []\n",
    "            for layer_features in batch['sae_features']:\n",
    "                # Transpose to get [num_features, batch_size]\n",
    "                features_t = layer_features.t()\n",
    "                for feature_idx in range(features_t.size(0)):\n",
    "                    all_features.append(features_t[feature_idx])\n",
    "            \n",
    "            # Stack into tensor [num_features, batch_size] \n",
    "            feature_tensor = torch.stack(all_features, dim=0)\n",
    "            \n",
    "            # Create fully connected directed graph between all features\n",
    "            # (L1 regularization will prune this during training)\n",
    "            edge_index = create_fully_connected_edges(len(all_features))\n",
    "            \n",
    "            # Forward pass through GNN\n",
    "            predicted_logits = gnn_model(feature_tensor, edge_index)\n",
    "            \n",
    "            # Compute loss: KL divergence to match distribution + L1 regularization\n",
    "            logit_loss = F.kl_div(\n",
    "                F.log_softmax(predicted_logits, dim=-1),\n",
    "                F.softmax(batch['logits'], dim=-1),\n",
    "                reduction='batchmean'\n",
    "            )\n",
    "            \n",
    "            # L1 regularization on attention weights\n",
    "            l1_loss = 0\n",
    "            for name, param in gnn_model.named_parameters():\n",
    "                if 'att' in name:  # GAT attention weights\n",
    "                    l1_loss += torch.sum(torch.abs(param))\n",
    "            \n",
    "            # Total loss\n",
    "            loss = logit_loss + l1_weight * l1_loss\n",
    "            \n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(training_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbdd184-30e0-466b-907c-a997cf66279f",
   "metadata": {},
   "source": [
    "#### Example data for each batch:\n",
    "\n",
    "```\n",
    "{\n",
    "    'input_ids': tensor[batch_size, seq_len],  # Input token IDs\n",
    "    'sae_features': [\n",
    "        # List of tensors, one per SAE layer\n",
    "        tensor[batch_size, num_features_layer1],  # Activations from SAE layer 1\n",
    "        tensor[batch_size, num_features_layer2],  # Activations from SAE layer 2\n",
    "        # ...and so on for all SAE layers\n",
    "    ],\n",
    "    'logits': tensor[batch_size, vocab_size]  # Original model output logits\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ac005-9a08-4dce-bb9f-80774dee26b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
