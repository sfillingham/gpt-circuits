{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "00212316-693f-4872-b21d-b222ebb6dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from safetensors.torch import load_model\n",
    "import json\n",
    "\n",
    "import torch as t\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "#from jaxtyping import Float, Int\n",
    "\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.utils import get_act_name, test_prompt, to_numpy\n",
    "\n",
    "\n",
    "\n",
    "# Path setup\n",
    "#project_root = Path(__file__).parent.parent\n",
    "#sys.path.append(str(project_root))\n",
    "\n",
    "# Imports from the project\n",
    "from config.sae.models import SAEConfig\n",
    "from models.sparsified import SparsifiedGPT\n",
    "from models.gpt import GPT\n",
    "\n",
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "65d844ae-341c-4907-b33b-2fa83e6e217d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT model...\n",
      "Loading SAE configuration...\n",
      "Creating SparsifiedGPT model...\n",
      "Loading SAE weights...\n"
     ]
    }
   ],
   "source": [
    "# Loading SparsifiedGPT\n",
    "device = torch.device(\"cpu\")\n",
    "checkpoint_dir = Path('/Volumes/MacMini/gpt-circuits') / \"checkpoints\"\n",
    "gpt_dir = checkpoint_dir / \"shakespeare_64x4\"\n",
    "sae_dir = checkpoint_dir / \"standard.shakespeare_64x4\"\n",
    "    \n",
    "# Load GPT model\n",
    "print(\"Loading GPT model...\")\n",
    "gpt = GPT.load(gpt_dir, device=device)\n",
    "    \n",
    "# Load SAE config\n",
    "print(\"Loading SAE configuration...\")\n",
    "sae_config_dir = sae_dir / \"sae.json\"\n",
    "with open(sae_config_dir, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "config = SAEConfig(**meta)\n",
    "config.gpt_config = gpt.config\n",
    "    \n",
    "# Create model using saved config\n",
    "print(\"Creating SparsifiedGPT model...\")\n",
    "model = SparsifiedGPT(config)\n",
    "model.gpt = gpt\n",
    "    \n",
    "# Load SAE weights\n",
    "print(\"Loading SAE weights...\")\n",
    "for layer_name, module in model.saes.items():\n",
    "    weights_path = os.path.join(sae_dir, f\"sae.{layer_name}.safetensors\")\n",
    "    load_model(module, weights_path, device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "004210e5-abf4-44f0-9116-a79be3deb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\",)))\n",
    "#print(\"\\n\".join(sys.path))\n",
    "# %%\n",
    "current_dir = os.path.dirname(os.path.abspath(\"spar_sae_circuit_sandbox.ipynb\"))\n",
    "model_dir = os.path.join(current_dir, '..') # Assuming it's one level up\n",
    "#toy_model_dir = os.path.join(current_dir, '..', 'llm_from_scratch/LLM_from_scratch/')\n",
    "\n",
    "sys.path.append(model_dir)\n",
    "#sys.path.append(toy_model_dir)\n",
    "\n",
    "from config.gpt.training import options\n",
    "from config.sae.models import sae_options\n",
    "from models.gpt import GPT\n",
    "from models.sparsified import SparsifiedGPT\n",
    "from data.tokenizers import ASCIITokenizer, TikTokenTokenizer\n",
    "\n",
    "#from utils import generate\n",
    "c_name = 'standardx8.shakespeare_64x4'\n",
    "name = 'standard.shakespeare_64x4'\n",
    "#name = 'shakespeare_64x4'\n",
    "config = sae_options[c_name]\n",
    "\n",
    "model = SparsifiedGPT(config)\n",
    "model_path = os.path.join(\"../checkpoints\", name)\n",
    "model = model.load(model_path, device=config.device)\n",
    "\n",
    "tokenizer = ASCIITokenizer() if \"shake\" in name else TikTokenTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "df68d384-4781-4d04-b748-dad500984e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_length=50, temperature=0.7) -> str:\n",
    "    \"\"\"\n",
    "    Generate text from a prompt using the model\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.Tensor(tokens).long().unsqueeze(0)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        logits = model(tokens).logits[0][-1]\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        #next_token = torch.multinomial(probs, num_samples=1)\n",
    "        next_token = torch.argmax(probs, keepdim=True)\n",
    "        \n",
    "        tokens = torch.cat([tokens.squeeze(0), next_token], dim=-1).unsqueeze(0)\n",
    "        \n",
    "    #return tokenizer.decode_sequence(tokens[0].tolist())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "44fdabca-2880-4642-a356-7b930df485e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "tensor([[ 72, 105, 115,  32, 110,  97, 109, 101,  32, 105, 115,  32,  76, 105,\n",
      "          99, 105, 111,  44,  32,  98, 111, 114, 110,  32, 105, 110,  32,  77,\n",
      "          97, 110, 116, 117,  97,  46,  10]])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"His name is Licio, born in Mantua.\"\n",
    "print(len(prompt))\n",
    "output = generate(model, tokenizer, prompt, max_length=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "a3d78154-3a77-4783-a32d-515e925f5ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New line character is 10 in AsciiTokenizer\n",
    "## Space character is 32 in AsciiTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "9751fe8b-21f1-47a5-a49d-14bf4eeecf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10]])"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_prompt = \"\\n\"\n",
    "tokens = tokenizer.encode(random_prompt)\n",
    "tokens = torch.Tensor(tokens).long().unsqueeze(0)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "9eedb462-1b42-4487-8f7b-0f8ab303177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "da75bacb-c66d-4e92-b616-277abf7972a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with model.use_saes():\n",
    "    #output_sae = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "f90ec079-e282-4026-bcde-23fa55a30e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logits', 'cross_entropy_loss', 'activations', 'ce_loss_increases', 'compound_ce_loss_increase', 'sae_loss_components', 'feature_magnitudes', 'reconstructed_activations'])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(output).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "e187ab64-b0c9-4e32-8858-747b417d8de0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.activations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "1c4291c1-f271-449b-b95c-9298c25dd42a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.feature_magnitudes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "f6c10923-72e3-4697-839a-301e038d3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature layers\n",
    "feat_layer0 = output.feature_magnitudes[0].squeeze(0)\n",
    "feat_layer1 = output.feature_magnitudes[1].squeeze(0)\n",
    "feat_layer2 = output.feature_magnitudes[2].squeeze(0)\n",
    "feat_layer3 = output.feature_magnitudes[3].squeeze(0)\n",
    "feat_layer4 = output.feature_magnitudes[4].squeeze(0)\n",
    "\n",
    "#minimum value a feature can be considered \"active\"\n",
    "feat_threshold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "c5105ff4-c31a-4f7f-8923-15566bd0181a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(feat_layer0 > feat_threshold)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "3882fb0d-2053-407d-a7c4-b8d8d57625af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Shakespeare validation data\n",
    "file_path = '/Volumes/MacMini/gpt-circuits/data/shakespeare/val_000000.npy'\n",
    "#with open(file_path) as file:\n",
    "val_input = np.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "57e86e8e-a667-4e5b-ae8e-b1bd7681ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the locations of the output logit of interest\n",
    "#\n",
    "new_line_indices = np.where(val_input == 10)[0]\n",
    "left_cut = 0\n",
    "right_cut = 0\n",
    "nl_prompt_list = []\n",
    "dl_prompt_list = []\n",
    "\n",
    "for i, idx in enumerate(new_line_indices):\n",
    "    right_cut = idx\n",
    "    if right_cut == left_cut:\n",
    "        continue\n",
    "    elif right_cut - left_cut == 1:\n",
    "        left_cut = right_cut\n",
    "        continue\n",
    "    else:\n",
    "        #Grab the sequence between newline characters plus the next two characters to check if its a double line\n",
    "        token_sequence = val_input[left_cut+1:right_cut+2]\n",
    "        if token_sequence[-1] == 10:\n",
    "            dl_prompt_list.append(tokenizer.decode_sequence(token_sequence[:-2]))\n",
    "        else:\n",
    "            #if len(token_sequence) >= 10:\n",
    "            nl_prompt_list.append(tokenizer.decode_sequence(token_sequence[:-2]))\n",
    "        left_cut = right_cut\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "5ec18a30-c8a9-4eb5-ba69-84f79336bb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then show it me.\n",
      "16\n",
      "'Tis with cares.\n",
      "16\n",
      "Call them forth.\n",
      "16\n",
      "countenance her.\n",
      "16\n",
      "How now, Grumio!\n",
      "16\n",
      "a puppet of her.\n",
      "16\n",
      "And what of him?\n",
      "16\n",
      "Is't so, indeed.\n",
      "16\n",
      "Roundly replied.\n",
      "16\n",
      "Who shall begin?\n",
      "16\n",
      "die a dry death.\n",
      "16\n",
      "Heaviness in me.\n",
      "16\n",
      "To the syllable.\n",
      "16\n",
      "Done. The wager?\n",
      "16\n",
      "beyond credit,--\n",
      "16\n",
      "how you take it!\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "sample_nl_prompt_list = []\n",
    "for prompt in nl_prompt_list:\n",
    "    if len(prompt) == 16:\n",
    "        #print(prompt)\n",
    "        #print(len(tokenizer.encode(prompt)))\n",
    "        sample_nl_prompt_list.append(prompt)\n",
    "\n",
    "sample_dl_prompt_list = []\n",
    "for prompt in dl_prompt_list:\n",
    "    if len(prompt) == 16:\n",
    "        print(prompt)\n",
    "        print(len(tokenizer.encode(prompt)))\n",
    "        sample_dl_prompt_list.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "7ba8b2ce-2300-4a6f-9fcf-462836348a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = []\n",
    "for prompt in sample_dl_prompt_list:\n",
    "    output = generate(model, tokenizer, prompt, max_length=1)\n",
    "    model_output.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "95c56228-5b89-4c63-bca2-45bc67ab1b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 84, 104, 101, 110,  32, 115, 104, 111, 119,  32, 105, 116,  32, 109,\n",
       "          101,  46,  10]]),\n",
       " tensor([[ 39,  84, 105, 115,  32, 119, 105, 116, 104,  32,  99,  97, 114, 101,\n",
       "          115,  46,  10]]),\n",
       " tensor([[ 67,  97, 108, 108,  32, 116, 104, 101, 109,  32, 102, 111, 114, 116,\n",
       "          104,  46,  10]]),\n",
       " tensor([[ 99, 111, 117, 110, 116, 101, 110,  97, 110,  99, 101,  32, 104, 101,\n",
       "          114,  46,  10]]),\n",
       " tensor([[ 72, 111, 119,  32, 110, 111, 119,  44,  32,  71, 114, 117, 109, 105,\n",
       "          111,  33,  32]]),\n",
       " tensor([[ 97,  32, 112, 117, 112, 112, 101, 116,  32, 111, 102,  32, 104, 101,\n",
       "          114,  46,  10]]),\n",
       " tensor([[ 65, 110, 100,  32, 119, 104,  97, 116,  32, 111, 102,  32, 104, 105,\n",
       "          109,  63,  10]]),\n",
       " tensor([[ 73, 115,  39, 116,  32, 115, 111,  44,  32, 105, 110, 100, 101, 101,\n",
       "          100,  46,  10]]),\n",
       " tensor([[ 82, 111, 117, 110, 100, 108, 121,  32, 114, 101, 112, 108, 105, 101,\n",
       "          100,  46,  10]]),\n",
       " tensor([[ 87, 104, 111,  32, 115, 104,  97, 108, 108,  32,  98, 101, 103, 105,\n",
       "          110,  63,  10]]),\n",
       " tensor([[100, 105, 101,  32,  97,  32, 100, 114, 121,  32, 100, 101,  97, 116,\n",
       "          104,  46,  10]]),\n",
       " tensor([[ 72, 101,  97, 118, 105, 110, 101, 115, 115,  32, 105, 110,  32, 109,\n",
       "          101,  46,  10]]),\n",
       " tensor([[ 84, 111,  32, 116, 104, 101,  32, 115, 121, 108, 108,  97,  98, 108,\n",
       "          101,  46,  10]]),\n",
       " tensor([[ 68, 111, 110, 101,  46,  32,  84, 104, 101,  32, 119,  97, 103, 101,\n",
       "          114,  63,  10]]),\n",
       " tensor([[ 98, 101, 121, 111, 110, 100,  32,  99, 114, 101, 100, 105, 116,  44,\n",
       "           45,  45,  10]]),\n",
       " tensor([[104, 111, 119,  32, 121, 111, 117,  32, 116,  97, 107, 101,  32, 105,\n",
       "          116,  33,  10]])]"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "6eaa7bcc-6a93-4cc4-b401-59ede1c00e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 84, 104, 101, 110,  32, 115, 104, 111, 119,  32, 105, 116,  32, 109,\n",
      "        101,  46])\n",
      "tensor([ 39,  84, 105, 115,  32, 119, 105, 116, 104,  32,  99,  97, 114, 101,\n",
      "        115,  46])\n",
      "tensor([ 67,  97, 108, 108,  32, 116, 104, 101, 109,  32, 102, 111, 114, 116,\n",
      "        104,  46])\n",
      "tensor([ 99, 111, 117, 110, 116, 101, 110,  97, 110,  99, 101,  32, 104, 101,\n",
      "        114,  46])\n",
      "tensor([ 97,  32, 112, 117, 112, 112, 101, 116,  32, 111, 102,  32, 104, 101,\n",
      "        114,  46])\n",
      "tensor([ 65, 110, 100,  32, 119, 104,  97, 116,  32, 111, 102,  32, 104, 105,\n",
      "        109,  63])\n",
      "tensor([ 73, 115,  39, 116,  32, 115, 111,  44,  32, 105, 110, 100, 101, 101,\n",
      "        100,  46])\n",
      "tensor([ 82, 111, 117, 110, 100, 108, 121,  32, 114, 101, 112, 108, 105, 101,\n",
      "        100,  46])\n",
      "tensor([ 87, 104, 111,  32, 115, 104,  97, 108, 108,  32,  98, 101, 103, 105,\n",
      "        110,  63])\n",
      "tensor([100, 105, 101,  32,  97,  32, 100, 114, 121,  32, 100, 101,  97, 116,\n",
      "        104,  46])\n",
      "tensor([ 72, 101,  97, 118, 105, 110, 101, 115, 115,  32, 105, 110,  32, 109,\n",
      "        101,  46])\n",
      "tensor([ 84, 111,  32, 116, 104, 101,  32, 115, 121, 108, 108,  97,  98, 108,\n",
      "        101,  46])\n",
      "tensor([ 68, 111, 110, 101,  46,  32,  84, 104, 101,  32, 119,  97, 103, 101,\n",
      "        114,  63])\n",
      "tensor([ 98, 101, 121, 111, 110, 100,  32,  99, 114, 101, 100, 105, 116,  44,\n",
      "         45,  45])\n",
      "tensor([104, 111, 119,  32, 121, 111, 117,  32, 116,  97, 107, 101,  32, 105,\n",
      "        116,  33])\n",
      "0.9375\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "output_list = []\n",
    "for output in model_output:\n",
    "    if output[0][-1] == 10:# and output[0][-2]:\n",
    "        count += 1\n",
    "        output_list.append(output[0][:-1])\n",
    "        print(output[0][:-1])\n",
    "\n",
    "print(count/len(model_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "ae647bb6-61ad-4520-a149-722a827822fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[642], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_list\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "output_list.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "2f524cd9-e5cf-4681-b8c8-2d7f3a3a7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(output_list, 'double_newline_prompts.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "e5b4d763-28a9-4ff5-9e0d-0ce89a064f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_newline_prompts = torch.load('double_newline_prompts.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "71f798b6-2540-45b7-89e6-a69c1b5158be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 84, 104, 101, 110,  32, 115, 104, 111, 119,  32, 105, 116,  32, 109,\n",
       "         101,  46]),\n",
       " tensor([ 39,  84, 105, 115,  32, 119, 105, 116, 104,  32,  99,  97, 114, 101,\n",
       "         115,  46]),\n",
       " tensor([ 67,  97, 108, 108,  32, 116, 104, 101, 109,  32, 102, 111, 114, 116,\n",
       "         104,  46]),\n",
       " tensor([ 99, 111, 117, 110, 116, 101, 110,  97, 110,  99, 101,  32, 104, 101,\n",
       "         114,  46]),\n",
       " tensor([ 97,  32, 112, 117, 112, 112, 101, 116,  32, 111, 102,  32, 104, 101,\n",
       "         114,  46]),\n",
       " tensor([ 65, 110, 100,  32, 119, 104,  97, 116,  32, 111, 102,  32, 104, 105,\n",
       "         109,  63]),\n",
       " tensor([ 73, 115,  39, 116,  32, 115, 111,  44,  32, 105, 110, 100, 101, 101,\n",
       "         100,  46]),\n",
       " tensor([ 82, 111, 117, 110, 100, 108, 121,  32, 114, 101, 112, 108, 105, 101,\n",
       "         100,  46]),\n",
       " tensor([ 87, 104, 111,  32, 115, 104,  97, 108, 108,  32,  98, 101, 103, 105,\n",
       "         110,  63]),\n",
       " tensor([100, 105, 101,  32,  97,  32, 100, 114, 121,  32, 100, 101,  97, 116,\n",
       "         104,  46]),\n",
       " tensor([ 72, 101,  97, 118, 105, 110, 101, 115, 115,  32, 105, 110,  32, 109,\n",
       "         101,  46]),\n",
       " tensor([ 84, 111,  32, 116, 104, 101,  32, 115, 121, 108, 108,  97,  98, 108,\n",
       "         101,  46]),\n",
       " tensor([ 68, 111, 110, 101,  46,  32,  84, 104, 101,  32, 119,  97, 103, 101,\n",
       "         114,  63]),\n",
       " tensor([ 98, 101, 121, 111, 110, 100,  32,  99, 114, 101, 100, 105, 116,  44,\n",
       "          45,  45]),\n",
       " tensor([104, 111, 119,  32, 121, 111, 117,  32, 116,  97, 107, 101,  32, 105,\n",
       "         116,  33])]"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_newline_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92320ef4-6185-4fb8-9aa7-f14b4077cbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
