{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00212316-693f-4872-b21d-b222ebb6dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch as t\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import HTML, IFrame, clear_output, display\n",
    "#from jaxtyping import Float, Int\n",
    "\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor, nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens.utils import get_act_name, test_prompt, to_numpy\n",
    "\n",
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65d844ae-341c-4907-b33b-2fa83e6e217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_length=50, temperature=0.7) -> str:\n",
    "    \"\"\"\n",
    "    Generate text from a prompt using the model\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.Tensor(tokens).long().unsqueeze(0)\n",
    "    for _ in range(max_length):\n",
    "        logits = model(tokens)[0][:, -1, :]\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, next_token], dim=-1)\n",
    "    return tokenizer.decode_sequence(tokens[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "004210e5-abf4-44f0-9116-a79be3deb745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\",)))\n",
    "#print(\"\\n\".join(sys.path))\n",
    "# %%\n",
    "current_dir = os.path.dirname(os.path.abspath(\"spar_sae_circuit_sandbox.ipynb\"))\n",
    "model_dir = os.path.join(current_dir, '..') # Assuming it's one level up\n",
    "#toy_model_dir = os.path.join(current_dir, '..', 'llm_from_scratch/LLM_from_scratch/')\n",
    "\n",
    "sys.path.append(model_dir)\n",
    "#sys.path.append(toy_model_dir)\n",
    "\n",
    "from config.gpt.training import options\n",
    "from config.sae.models import sae_options\n",
    "from models.gpt import GPT\n",
    "from models.sparsified import SparsifiedGPT\n",
    "from data.tokenizers import ASCIITokenizer, TikTokenTokenizer\n",
    "\n",
    "#from utils import generate\n",
    "c_name = 'standardx8.shakespeare_64x4'\n",
    "name = 'standard.shakespeare_64x4'\n",
    "#name = 'shakespeare_64x4'\n",
    "config = sae_options[c_name]\n",
    "\n",
    "model = SparsifiedGPT(config)\n",
    "model_path = os.path.join(\"../checkpoints\", name)\n",
    "model = model.load(model_path, device=config.device)\n",
    "\n",
    "tokenizer = ASCIITokenizer() if \"shake\" in name else TikTokenTokenizer()\n",
    "\n",
    "#print(generate(model, tokenizer, \"Today I thought,\", max_length=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3d78154-3a77-4783-a32d-515e925f5ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t.set_grad_enabled(False)\n",
    "#hf_model = 'davidquarel/standard.shakespeare_64x4'\n",
    "#gpt2: HookedSAETransformer = HookedSAETransformer.from_pretrained(repo_id=hf_model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9751fe8b-21f1-47a5-a49d-14bf4eeecf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (0): StandardSAE()\n",
       "  (1): StandardSAE()\n",
       "  (2): StandardSAE()\n",
       "  (3): StandardSAE()\n",
       "  (4): StandardSAE()\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.saes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75bacb-c66d-4e92-b616-277abf7972a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ec079-e282-4026-bcde-23fa55a30e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
